import torch
import torch.nn as nn
from torch import distributions
from torch.optim import Adam, lr_scheduler
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm

from models.flow_modules import (
    CouplingLayer,
    AffineCouplingFunc,
    ConditionalNet,
    StraightNet,
)
from models.point_encoders import PointnetEncoder
from utils import loss_fun , loss_fun_ret, view_cloud

from data.datasets_pointflow import (
    CIFDatasetDecorator,
    ShapeNet15kPointClouds,
    CIFDatasetDecoratorMultiObject,
)
n_f= 10
n_f_k = 3
n_g = 3
n_g_k=2
n_epochs = 100
batch_size = 2
x_noise = 0.0001
random_dataloader = True
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
prior_z = distributions.MultivariateNormal(
    torch.zeros(3), torch.eye(3)
)
emb_dim = 32
prior_e = distributions.MultivariateNormal(
    torch.zeros(emb_dim), torch.eye(emb_dim)
)

cloud_pointflow = ShapeNet15kPointClouds(
    tr_sample_size=2048,
    te_sample_size=2048,
    root_dir='data\\ShapeNetCore.v2.PC15k',
  
    normalize_per_shape=False,
    normalize_std_per_axis=False,
    split="train",
    scale=1.0,
    categories=["airplane"],
    random_subsample=True,
)



if random_dataloader:
        cloud_pointflow = CIFDatasetDecoratorMultiObject(
            cloud_pointflow, 2048
        )
        batch_size = 2 #  original 50, down for mem
dataloader_pointflow = DataLoader(
    cloud_pointflow, batch_size=batch_size, shuffle=True
)


# Prepare models


pointnet = PointnetEncoder(emb_dim,input_dim=3).to(device)




# for f
f_blocks = [[] for x in range(n_f)]
f_permute_list_list = [[0,2,1],[2,0,1],[1,0,2]]
f_split_index_list = [1]*len(f_permute_list_list)

for i in range(n_f):
    for j in range(n_f_k):
        split_index = f_split_index_list[j]
        permute_tensor = torch.LongTensor([f_permute_list_list[j] ]).to(device)  

        mutiply_func = ConditionalNet(emb_dim=emb_dim,in_dim=split_index)
        add_func  = ConditionalNet(emb_dim=emb_dim,in_dim = split_index)
        coupling_func = AffineCouplingFunc(mutiply_func,add_func)
        coupling_layer = CouplingLayer(coupling_func,split_index,permute_tensor)
        f_blocks[i].append(coupling_layer)

# for g
g_blocks = [[] for x in range(n_g)]
g_permute_list_list = [list(range(emb_dim))[::-1],[x if x%2 ==0 else emb_dim - x for x in range(emb_dim) ]]
g_split_index_list = [emb_dim//2]*len(g_permute_list_list)

for i in range(n_g):
    for j in range(n_g_k):
        split_index = g_split_index_list[j]
        permute_tensor = torch.LongTensor([g_permute_list_list[j] ]).to(device)        
        mutiply_func = StraightNet(in_dim = split_index)
        add_func  = StraightNet(split_index)
        coupling_func = AffineCouplingFunc(mutiply_func,add_func)
        coupling_layer = CouplingLayer(coupling_func,split_index,permute_tensor)
        g_blocks[i].append(coupling_layer)        
   

model_dict = {'pointnet':pointnet}
for i, f_block in enumerate(f_blocks):
    for k in range(len(f_block)):
        model_dict[f'f_block_{i}_{k}'] = f_block[k]

for i, g_block in enumerate(g_blocks):
     for k in range(len(g_block)):
        model_dict[f'g_block_{i}_{k}'] = g_block[k]

all_params = []
for model_part in model_dict.values():
    #Send to device before passing to optimizer
    model_part.to(device)
    model_part.train()
    all_params += model_part.parameters()
optimizer = Adam(all_params,lr=1e-4)
lambda_func_scheduler = lambda x: 0.8**(x//10)
scheduler = lr_scheduler.LambdaLR(optimizer,lambda_func_scheduler)
for epoch in tqdm(range(n_epochs)):
    loss_acc_z = 0
    loss_acc_e = 0

    optimizer.zero_grad()
    for index, batch in enumerate(tqdm(dataloader_pointflow)):

        

        # The sampling that goes through pointnet
        embs_tr_batch = batch["train_points"].to(device)

        # The sampling that goes through f conditioned on e generated by the previous sampling
        tr_batch = batch['points_to_decode']

        #Add noise to tr_batch:
        tr_batch = tr_batch.float() + x_noise * torch.rand(tr_batch.shape)
        tr_batch = tr_batch.to(device)
        #Store before reshaping
        num_points_per_object = tr_batch.shape[1]
        #Squashing each shape into one dimension
       # tr_batch = tr_batch.to(device).reshape((-1, 3)) #get back to this
        
                

        #Pass through pointnet
        w = pointnet(embs_tr_batch)
        w = w.unsqueeze(dim=1)
        w = w.expand([w.shape[0],num_points_per_object,w.shape[-1]])

        #Expand/repeat the embedding w so that each point has its own
        # w_iter = w
        # w_iter = (
        #     w_iter.unsqueeze(dim=1)
        #     .expand(
        #         [w_iter.shape[0], num_points_per_object, w_iter.shape[-1]]
        #     )
        #     .reshape((-1, w_iter.shape[-1]))
        # )
            
        #Pass pointnet embedding through g flow and keep track of determinant
        e_ldetJ = 0
        e = w
        for g_block in g_blocks:
            for g_layer in g_block:
                e, inter_e_ldetJ = g_layer(e)
                e_ldetJ += inter_e_ldetJ
        
        #Pass pointcloud through f flow conditioned on e and keep track of determinant
        z_ldetJ=0
        z = tr_batch
        for f_block in f_blocks:
            for f_layer in f_block:
                z, inter_z_ldetJ = f_layer(z,e)
                z_ldetJ += inter_z_ldetJ
        
        
        loss_z, loss_e = loss_fun(
                z,
                z_ldetJ,
                prior_z,
                e,
                e_ldetJ,
                prior_e,
            )
        loss = loss_e + loss_z
        loss_acc_z += loss_z.item()
        loss_acc_e += loss_e.item()

        loss.backward()
        # Adjust lr according to epoch
        scheduler.step()